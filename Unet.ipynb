{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a10d0b2-93fa-4423-8880-ad8726cd475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "from rasterio.plot import reshape_as_image\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob,os,sys,cv2\n",
    "from datetime import datetime\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "training_path = \"Input/sentinel/test_data_from_drive/patches_all/train/\"\n",
    "target_file_path = \"Input/Target/concat/target_yield.shp\"\n",
    "patch_dim = (256, 256, 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "477e7a9d-1178-481d-b7ad-9010ca15bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "lr = 1e-4\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    def f(y_true, y_pred):\n",
    "        intersection = (y_true * y_pred).sum()\n",
    "        union = y_true.sum() + y_pred.sum() - intersection\n",
    "        x = (intersection + 1e-15) / (union + 1e-15)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e265cae-e649-4f37-82c5-db97379973a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "    # Conv2D then ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    # Conv2D then ReLU activation\n",
    "    x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"relu\", kernel_initializer = \"he_normal\")(x)\n",
    "    return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    p = layers.Dropout(0.3)(p)\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    # upsample\n",
    "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    # concatenate\n",
    "    x = layers.concatenate([x, conv_features])\n",
    "    # dropout\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    # Conv2D twice with ReLU activation\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b692c632-848f-4cb1-9b9d-a78154414fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Patches: 22707\n",
      "Any Null values?  False\n",
      "x shape :(101, 256, 256, 12), y shape: (101, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "def read_training():\n",
    "    training_file_list = glob.glob(os.path.join(training_path,\"*.tif\"))\n",
    "    target_gdf = gpd.read_file(target_file_path)\n",
    "    print(\"Total Number of Patches:\",len(training_file_list))\n",
    "    ignore_patch_list = list()\n",
    "    x = list()\n",
    "    y = list()\n",
    "    X_train = list()\n",
    "    X_test = list()\n",
    "    y_train = list()\n",
    "    y_test = list()\n",
    "    count = 0 \n",
    "    for file in training_file_list:\n",
    "\n",
    "        patch_src = rio.open(file)\n",
    "        f_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        patch_src_read = reshape_as_image(patch_src.read()) ## Change the index here to add or remove the mask layer\n",
    "        # print(0)\n",
    "        if patch_src_read.shape != patch_dim:\n",
    "            ignore_patch_list.append(f_name)\n",
    "            # print(\"Patch Dimensions Mismatch, skipping patch : {}\".format(f_name))\n",
    "            continue\n",
    "\n",
    "        # print(1)\n",
    "        if np.isnan(patch_src_read).any():\n",
    "            # print(\"Has Nan values, skipping patch : {}\".format(f_name))\n",
    "            continue\n",
    "\n",
    "        # print(2)\n",
    "        query = target_gdf.query(f\"patch_name == '{f_name}'\")[\"ykg_by_e7\"]\n",
    "        if len(query) != 1:\n",
    "            # print(\"patch has no target value, skipping patch : {}\".format(f_name))\n",
    "            continue\n",
    "        # print(patch_src_read[:,:,0:12].shape)\n",
    "        # print(patch_src_read[:,:,12].shape)\n",
    "\n",
    "        x.append(patch_src_read[:,:,0:12])\n",
    "        y.append(patch_src_read[:,:,12])\n",
    "        # y.append(float(query))\n",
    "\n",
    "        patch_src.close()\n",
    "        # print(count)\n",
    "        count +=1\n",
    "        if count > 100:\n",
    "            break\n",
    "\n",
    "    # self.y = self.scaler.fit_transform(np.array(self.y).reshape(-1, 1))\n",
    "    y = np.array(y)\n",
    "    y = np.expand_dims(y,-1)\n",
    "    x = np.array(x)\n",
    "    print(\"Any Null values? \",np.isnan(x).any())\n",
    "    # print(self.y)\n",
    "    # self.x = np.nan_to_num(self.x, nan=0)# Check for different value for no data\n",
    "    print(f\"x shape :{x.shape}, y shape: {y.shape}\")\n",
    "    # print(np.nanmin(self.x),np.nanmax(self.x))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    #Also, split the training into train and val\n",
    "    # For testing, keep a part of the dataset as seperate (final month)\n",
    "\n",
    "X_train, X_test, y_train, y_test = read_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1409fceb-b1f6-4a30-aba8-4bf2b6760c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_LENGTH = info.splits[\"train\"].num_examples\n",
    "STEPS_PER_EPOCH = len(X_train) // BATCH_SIZE\n",
    "VAL_SUBSPLITS = 5\n",
    "# TEST_LENTH = info.splits[\"test\"].num_examples\n",
    "VALIDATION_STEPS = len(X_test) // BATCH_SIZE // VAL_SUBSPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa471f6-a3ea-4630-b92b-f8cbb665de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 23:29:53.436180: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "def build_unet_model():\n",
    "    # inputs\n",
    "    inputs = layers.Input(shape=(256,256,12))\n",
    "\n",
    "    # encoder: contracting path - downsample\n",
    "    # 1 - downsample\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    # 2 - downsample\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    # 3 - downsample\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    # 4 - downsample\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "    # 5 - bottleneck\n",
    "    bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "    # decoder: expanding path - upsample\n",
    "    # 6 - upsample\n",
    "    u6 = upsample_block(bottleneck, f4, 512)\n",
    "    # 7 - upsample\n",
    "    u7 = upsample_block(u6, f3, 256)\n",
    "    # 8 - upsample\n",
    "    u8 = upsample_block(u7, f2, 128)\n",
    "    # 9 - upsample\n",
    "    u9 = upsample_block(u8, f1, 64)\n",
    "\n",
    "    # outputs\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"softmax\")(u9)\n",
    "\n",
    "    # unet model with Keras Functional API\n",
    "    unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "\n",
    "    return unet_model\n",
    "\n",
    "\n",
    "unet_model = build_unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b90660-8f92-4792-aa81-2cf736025493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_parse(x,y):\n",
    "    def f(x, y):\n",
    "        return x, y\n",
    "    images, masks = tf.numpy_function(f, [x, y], [tf.float32, tf.float32])\n",
    "    images.set_shape([256, 256, 12])\n",
    "    masks.set_shape([256, 256, 1])\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "def tf_dataset(x,y,batch=8):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.map(tf_parse)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    # break\n",
    "    return dataset\n",
    "\n",
    "train_dataset = tf_dataset(X_train,y_train,batch=BATCH_SIZE)\n",
    "validation_dataset = tf_dataset(X_test,y_test,batch=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "60a7c20c-1a73-4610-9197-d069f47018d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in train_dataset:\n",
    "#     i = i[0].numpy()\n",
    "#     j = j[0].numpy()\n",
    "#     j = np.squeeze(j,axis=-1)\n",
    "#     print(i.shape,j.shape)\n",
    "#     i=i*255\n",
    "#     j = j*255.0\n",
    "#     cv2.imwrite(\"img.png\",i[:,:,3:6])\n",
    "    \n",
    "#     cv2.imwrite(\"mask.png\",j)\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ecf4566-86e3-40a7-8840-92dd6d877f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"acc\", tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), iou]\n",
    "\n",
    "unet_model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=metrics)\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"unet_files/model_\"+str(datetime.now())+\".h5\"),\n",
    "    ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=3),\n",
    "    CSVLogger(\"unet_files/data_\"+str(datetime.now())+\".csv\"),\n",
    "    TensorBoard(),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f53b16cd-5485-4ff4-be10-bd6d9db14ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 60s 6s/step - loss: 0.5326 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5699 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 58s 6s/step - loss: 0.5492 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5518 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 58s 6s/step - loss: 0.5261 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5488 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 57s 6s/step - loss: 0.5267 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5450 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 58s 6s/step - loss: 0.5217 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5441 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-05\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 57s 6s/step - loss: 0.5211 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5432 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-05\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 58s 6s/step - loss: 0.5204 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5425 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-05\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 57s 6s/step - loss: 0.5198 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5424 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-06\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 58s 6s/step - loss: 0.5198 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5423 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-06\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 57s 6s/step - loss: 0.5198 - acc: 0.2226 - recall_2: 1.0000 - precision_2: 0.2226 - iou: 0.2177 - val_loss: 0.5423 - val_acc: 0.2547 - val_recall_2: 1.0000 - val_precision_2: 0.2547 - val_iou: 0.2755 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "model_history = unet_model.fit(train_dataset,\n",
    "                              validation_data = validation_dataset,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              callbacks=callbacks)\n",
    "                              # steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                              # validation_steps=VALIDATION_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
